{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "#from datasets import load_metric  not being used\n",
    "import torch\n",
    "from unidecode import unidecode\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 27 14:32:40 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.68.01    Driver Version: 512.59       CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   51C    P8    N/A /  N/A |      0MiB /  4096MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi  # not using cuda due to memory issues. (access to a 4GB ram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly analyze mBART-50, I chose an exisiting parallel [corpus](https://www.statmt.org/europarl/v7/fr-en.tgz) from [Europarl](https://www.statmt.org/europarl/). This corpus is in French and English and has already been aligned with the Church and Gale algorithm. There are a total of *2007723* sentences of which only 225 are used as the final dataset.\n",
    "\n",
    "For preprocessing, both unidecode and random_split from torch were used in initial runs. However, my current implementation of random_split with random seeds was still perturbing the alignment and as such I did not shuffle the sentence ordering. This is problematic in terms of evaluation, because we are passing the data in the same order. Alternatively, another split that could be considered for an extension of this project is what the [MT-Adapted Datasheets for Datasets](https://arxiv.org/pdf/2005.13156.pdf ) paper stated: *It is recommended to use data from the last quarter of 2000 as a test set, while the rest should be used as training data.* Nevertheless, this seems to be a heuristic and a general recommendation and as such, I still stuck to my \"ordinal\" split. To be more precise, 80% of my data went to the training set and the remaining 20% to a test set. The training set split was given such a big margin in case I'd like to add a validation set split. \n",
    "\n",
    "Additionally, unidecode was needed to standardize certain characters such as quotation marks and dashes '-'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'  # forcing cpu. I left other device statement for runs on the cloud\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading mBART's model, tokenizer, and passing language values\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer.src_lang = \"fr_XX\"\n",
    "tokenizer.tgt_lang = \"en_XX\"\n",
    "\n",
    "# Calling corpora files. It is expected that they are in the root dir of this notebook.\n",
    "\n",
    "src_file = \"europarl-v7.fr-en.fr\"\n",
    "trg_file = \"europarl-v7.fr-en.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(corpus_file, n=225):\n",
    "    \"\"\"\n",
    "    :param corpus_file: file\n",
    "    :param n: number of sentences to be fetched. The default is 225\n",
    "    Opens corpus file. tokenization is not called after reading the file if the user needs to \n",
    "    perform any custom data splits\n",
    "    returns: 225 sentences out of the corpus\n",
    "    \"\"\"\n",
    "    with open(corpus_file, 'r', encoding=\"utf-8\") as f:\n",
    "        corpus = f.readlines()\n",
    "    f.close()\n",
    "    return corpus[:n]\n",
    "\n",
    "def tokenize_set(corpus):  # 500\n",
    "    \"\"\"\n",
    "    :param n: corpus\n",
    "    returns: tokenized sentences in a list. Each tokenized sentence represents a dictionary with input_ids\n",
    "    and attention masks\n",
    "    \"\"\"   \n",
    "    return [tokenizer(unidecode(line), return_tensors=\"pt\", padding=True) for line in corpus]\n",
    "\n",
    "def translate(sentences, trg_lang_code=tokenizer.tgt_lang, model=model):\n",
    "    \"\"\"\n",
    "    : param sentences: a list containing tokenized sentences\n",
    "    : trg_lang_code: a string with the target language's code. Default is the tokenizer's tgt_lang attribute. I.e. en_XX\n",
    "    returns: decoded tokens of a translated sentence. I.e. a translation\n",
    "    \"\"\"\n",
    "    translated_sents = list()\n",
    "    for sent in sentences:\n",
    "        # mBART50 may accept a max size of 512 or 1024. Beam size can also be played around with.\n",
    "        generated_tokens = model.generate(**sent, num_beams=1, max_length=512, \n",
    "                    forced_bos_token_id=tokenizer.lang_code_to_id[trg_lang_code])\n",
    "        decoded_tokens = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        translated_sents.append(decoded_tokens)\n",
    "    return translated_sents\n",
    "\n",
    "def idtensor_to_tokens(tensor):\n",
    "    \"\"\"\n",
    "    : param tensor: a tensor \n",
    "    it is flattened and cast to...\n",
    "    returns: convert ids to tokens\n",
    "    \"\"\"\n",
    "    flattened = tensor.squeeze().detach().numpy()\n",
    "    return tokenizer.convert_ids_to_tokens(flattened)\n",
    "\n",
    "def iterate_tensors(tokenized_sents):\n",
    "    \"\"\"\n",
    "    : param tokenized_sents: expects tokenization with the encoded tokens. Dictionary with 2 keys, input_ids and attention mask\n",
    "    returns: nested encoded tokens (input_ids)\n",
    "    \"\"\"\n",
    "    return [idtensor_to_tokens(t[\"input_ids\"]) for t in tokenized_sents]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up corpus and zip it in a list\n",
    "parallel_corpus = list(zip(open_file(src_file, n=225), open_file(trg_file, n=225)))\n",
    "# split corpus\n",
    "train_set_size = int(len(parallel_corpus) * 0.8)  # value can be changed to change data splits\n",
    "test_set_size = len(parallel_corpus) - train_set_size\n",
    "train_set = parallel_corpus[:train_set_size]\n",
    "test_set = parallel_corpus[train_set_size:]\n",
    "# random_split no longer being used but left in final version for future testing\n",
    "#train_set, test_set = random_split(parallel_corpus, [train_set_size, test_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# unzipping the data splits into source and target sentences. Doing this with the current random_split would \n",
    "# disturb alignment\n",
    "src_train_sents, trg_train_sents = zip(*train_set)\n",
    "src_test_sents, trg_test_sents = zip(*test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "180\n",
      "('Reprise de la session\\n', 'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\\n', 'Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\\n')\n",
      "('Resumption of the session\\n', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\n', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\n\")\n"
     ]
    }
   ],
   "source": [
    "# Showing size of train split. \n",
    "print(len(src_train_sents))\n",
    "print(len(trg_train_sents))\n",
    "print(src_train_sents[:3])\n",
    "print(trg_train_sents[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "\n",
    "By using the previously written tokenization function, I tokenized all of the source and target sentences. We convert the ids to tokens to visualize the sentences. Note that the first index corresponds to the language code. Since this step only required to perform translations with the test set, the example corresponds to the first sentence in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[250008, 125876,  72581,  10274,     40,      8,  88500,  16865,      8,\n",
      "           5460,  40276,  40514,      4,  10274,  43529,  72581,     21,  54159,\n",
      "           1505,   1609,    104,     25,  19988,  40276,  40514,      7,      4,\n",
      "            405,    569,  16095,  81581,    773,   6432,      8, 123196,      5,\n",
      "              6,      2]])\n",
      "\n",
      "['fr_XX', '▁Elles', '▁vont', '▁soit', '▁se', '▁de', 'barra', 'sser', '▁de', '▁leur', '▁carga', 'ison', ',', '▁soit', '▁elles', '▁vont', '▁la', '▁melan', 'ger', '▁avec', '▁d', \"'\", 'autres', '▁carga', 'ison', 's', ',', '▁ce', '▁qui', '▁eng', 'endre', '▁une', '▁serie', '▁de', '▁problemes', '.', '▁', '</s>']\n",
      "\n",
      "tensor([[250008,  32255, 164917,  53095,  40101,  60458,    111,   2363,  33362,\n",
      "            707,  17664,    442,    678,   3789,  33362,      4,   3129, 113660,\n",
      "          44402,      5,      6,      2]])\n",
      "\n",
      "['fr_XX', '▁These', '▁smaller', '▁companies', '▁either', '▁dispose', '▁of', '▁their', '▁cargo', '▁or', '▁mix', '▁it', '▁with', '▁other', '▁cargo', ',', '▁which', '▁causes', '▁problems', '.', '▁', '</s>']\n"
     ]
    }
   ],
   "source": [
    "src = tokenize_set(src_test_sents)\n",
    "trg = tokenize_set(trg_test_sents)\n",
    "src_sents_from_ids = iterate_tensors(src)\n",
    "eval_sents_from_ids = iterate_tensors(trg)\n",
    "print(src[0][\"input_ids\"])\n",
    "print()\n",
    "print(src_sents_from_ids[0])\n",
    "print()\n",
    "print(trg[0][\"input_ids\"])\n",
    "print()\n",
    "print(eval_sents_from_ids[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing translations once, I stored them in a file to facilitate testing and debugging. Note that to this submission the *translations_output.txt* has been attached. Otherwise, the translate function may be run locally. As a final note, in terms of writing the file, the *writelines()* method may be a more efficient way.\n",
    "```Python\n",
    "translations = translate(src)\n",
    "\n",
    "with open(\"translation_output.txt\", 'w') as f:\n",
    "    for trans in translations:        \n",
    "        f.write(trans[0] + '\\n')  # writelines()\n",
    "f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the translation file has been provided, we simply load them up.\n",
    "\n",
    "with open(\"translation_output_45_sents.txt\", 'r') as f:\n",
    "    translations = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence ['fr_XX', '▁Mais', '▁je', '▁tien', 's', '▁a', '▁dire', ',', '▁Mes', 'da', 'mes', '▁et', '▁Messi', 'eurs', '▁les', '▁Deput', 'es', ',', '▁que', '▁la', '▁securi', 'te', '▁est', '▁un', '▁', 'objectif', '▁priorit', 'aire', '▁de', '▁la', '▁Commission', '.', '▁', '</s>']\n",
      "\n",
      "Manual translation from Eurparl: ['fr_XX', '▁But', '▁I', '▁would', '▁like', '▁to', '▁say', '▁that', '▁safety', '▁is', '▁a', '▁priorit', 'y', '▁objective', '▁for', '▁the', '▁Commission', '.', '▁', '</s>']\n",
      "\n",
      "mBART translation ['fr_XX', '▁But', '▁I', '▁would', '▁like', '▁to', '▁say', ',', '▁la', 'dies', '▁and', '▁gent', 'le', 'men', ',', '▁that', '▁security', '▁is', '▁a', '▁priorit', 'y', '▁objective', '▁of', '▁the', '▁Commission', '.', '▁', '</s>']\n",
      "\n",
      "\n",
      "\n",
      "Original sentence ['fr_XX', '▁Comme', '▁je', '▁le', '▁dira', 'i', '▁lors', '▁du', '▁debat', '▁sur', '▁l', \"'\", 'E', 'rika', ',', '▁nous', '▁n', \"'\", 'attend', 'ons', '▁pas', '▁qu', \"'\", 'une', '▁cata', 'stro', 'phe', '▁sur', 'vien', 'ne', '▁pour', '▁nous', '▁con', 's', 'ac', 'rer', '▁corps', '▁et', '▁ame', '▁a', '▁l', \"'\", 'aspect', '▁de', '▁la', '▁securi', 'te', ',', '▁mais', '▁nous', '▁travail', 'lon', 's', '▁dessus', '▁en', '▁marge', '▁de', '▁ces', '▁circonstances', ',', '▁qui', '▁ne', '▁font', '▁que', '▁mettre', '▁en', '▁evidence', '▁l', \"'\", 'urgence', '▁d', \"'\", 'une', '▁repo', 'nse', '▁efficace', '▁a', '▁ce', '▁type', '▁de', '▁problemes', '.', '▁', '</s>']\n",
      "\n",
      "Manual translation from Eurparl: ['fr_XX', '▁As', '▁I', '▁will', '▁say', '▁in', '▁the', '▁debate', '▁on', '▁the', '▁Erik', 'a', '▁disa', 'ster', ',', '▁we', '▁do', '▁not', '▁wait', '▁until', '▁there', '▁is', '▁a', '▁disa', 'ster', '▁to', '▁deal', '▁with', '▁the', '▁question', '▁of', '▁safety', ',', '▁but', '▁we', '▁work', '▁on', '▁it', '▁even', '▁when', '▁there', '▁are', '▁no', '▁such', '▁circumstances', ',', '▁which', '▁simply', '▁serve', '▁to', '▁demonstrat', 'e', '▁the', '▁urge', 'ncy', '▁for', '▁an', '▁effective', '▁response', '▁to', '▁this', '▁type', '▁of', '▁problem', '.', '▁', '</s>']\n",
      "\n",
      "mBART translation ['fr_XX', '▁As', '▁I', '▁said', '▁in', '▁the', '▁Erik', 'a', '▁debate', ',', '▁we', '▁do', '▁not', '▁expect', '▁a', '▁disa', 'ster', '▁to', '▁happen', '▁to', '▁devo', 'te', '▁our', '▁bo', 'dies', '▁and', '▁our', '▁security', ',', '▁but', '▁we', '▁are', '▁working', '▁on', '▁it', '▁in', '▁the', '▁margin', 's', '▁of', '▁these', '▁circumstances', ',', '▁which', '▁only', '▁highlight', '▁the', '▁urge', 'ncy', '▁of', '▁an', '▁effective', '▁response', '▁to', '▁these', '▁kind', 's', '▁of', '▁problems', '.', '▁', '</s>']\n",
      "\n",
      "\n",
      "\n",
      "Original sentence ['fr_XX', '▁Je', '▁tien', 's', '▁a', '▁re', 'iter', 'er', '▁mes', '▁remercie', 'ments', '▁a', '▁tous', '▁les', '▁interven', 'ants', '▁et', '▁plus', '▁particulier', 'ement', '▁au', '▁rapport', 'eur', ',', '▁M', '.', '▁Koch', '.', '▁', '</s>']\n",
      "\n",
      "Manual translation from Eurparl: ['fr_XX', '▁I', '▁would', '▁like', '▁to', '▁repeat', '▁my', '▁ap', 'preci', 'ation', '▁to', '▁all', '▁the', '▁speaker', 's', '▁and', '▁especially', '▁to', '▁the', '▁rapport', 'eur', ',', '▁Mr', '▁Koch', '.', '▁', '</s>']\n",
      "\n",
      "mBART translation ['fr_XX', '▁I', '▁would', '▁like', '▁to', '▁reitera', 'te', '▁my', '▁thanks', '▁to', '▁all', '▁the', '▁speaker', 's', '▁and', '▁in', '▁particular', '▁to', '▁the', '▁rapport', 'eur', ',', '▁Mr', '▁Koch', '.', '▁', '</s>']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hypotheses = [tokenizer(sent, return_tensors=\"pt\") for sent in translations]  # do I need the tensors?\n",
    "hypotheses = iterate_tensors(hypotheses)\n",
    "references = eval_sents_from_ids\n",
    "original_french_sents = src_sents_from_ids\n",
    "# outputting 3 sample translations\n",
    "for i in range(40, 43):  # since we used the test split, i.e. 20% of total data set (225), we only have 45 sentences.\n",
    "    print(f\"Original sentence {original_french_sents[i]}\" + '\\n')\n",
    "    print(f\"Manual translation from Eurparl: {references[i]}\" + '\\n' * 2 + f\"mBART translation {hypotheses[i]}\" + '\\n' * 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr_XX', '▁I', '▁would', '▁like', '▁to', '▁mention', '▁one', '▁final', '▁point', '.', '▁', '</s>']\n",
      "['fr_XX', '▁I', '▁would', '▁like', '▁to', '▁raise', '▁one', '▁last', '▁point', '.', '▁', '</s>']\n",
      "\n",
      "['fr_XX', '▁I', '▁should', '▁like', '▁to', '▁make', '▁just', '▁a', '▁few', '▁comments', '.', '▁', '</s>']\n",
      "['fr_XX', '▁I', '▁would', '▁like', '▁to', '▁make', '▁a', '▁few', '▁re', 'marks', '.', '▁', '</s>']\n",
      "\n",
      "['fr_XX', '▁Consider', 'ing', '▁that', '▁it', '▁is', '▁only', '▁today', '▁that', '▁we', '▁are', '▁dealing', '▁with', '▁a', '▁Commission', '▁proposal', '▁first', '▁made', '▁on', '▁19', '▁March', '▁1998', ',', '▁even', '▁though', '▁Parliament', '▁responde', 'd', '▁relative', 'ly', '▁quickly', ',', '▁this', '▁time', '▁lag', '▁is', '▁a', '▁little', '▁too', '▁long', '.', '▁', '</s>']\n",
      "['fr_XX', '▁When', '▁I', '▁consider', '▁that', '▁the', '▁first', '▁Commission', '▁proposal', '▁was', '▁table', 'd', '▁on', '▁19', '▁March', '▁1998', '▁and', '▁that', '▁we', '▁are', '▁now', '▁dealing', '▁with', '▁it', '▁-', '▁when', '▁Parliament', '▁react', 'ed', '▁quite', '▁quickly', '▁-', '▁the', '▁de', 'lay', '▁is', '▁a', '▁little', '▁too', '▁long', '.', '▁', '</s>']\n",
      "\n",
      "['fr_XX', '▁In', '▁princip', 'le', ',', '▁I', '▁believe', '▁that', '▁in', '▁many', '▁cases', '▁where', '▁transport', '▁is', '▁concerned', '▁we', '▁should', '▁be', '▁working', '▁towards', '▁increased', '▁flexibil', 'ity', '▁and', '▁country', '-', 'specific', '▁rules', '.', '▁', '</s>']\n",
      "['fr_XX', '▁Fundamental', 'ly', ',', '▁I', '▁believe', '▁that', '▁in', '▁many', '▁areas', '▁of', '▁transport', '▁we', '▁should', '▁a', 'im', '▁for', '▁greater', '▁flexibil', 'ity', '▁and', '▁country', '-', 'by', '-', 'count', 'ry', '▁regulations', '.', '▁', '</s>']\n",
      "\n",
      "['fr_XX', '▁Should', '▁flexibil', 'ity', '▁of', '▁this', '▁kind', '▁result', '▁in', '▁there', '▁being', '▁in', 'adequa', 'te', '▁rules', '▁in', '▁some', '▁countries', '▁then', '▁we', '▁should', '▁work', '▁towards', '▁greater', '▁harmoni', 's', 'ation', '.', '▁', '</s>']\n",
      "['fr_XX', '▁If', '▁this', '▁kind', '▁of', '▁flexibil', 'ity', '▁should', '▁lead', '▁to', '▁uns', 'atis', 'factor', 'y', '▁regulations', '▁in', '▁some', '▁countries', ',', '▁we', '▁should', '▁opt', '▁for', '▁greater', '▁harmoni', 's', 'ation', '.', '▁', '</s>']\n",
      "\n",
      "['fr_XX', '▁I', '▁would', '▁like', '▁to', '▁repeat', '▁my', '▁ap', 'preci', 'ation', '▁to', '▁all', '▁the', '▁speaker', 's', '▁and', '▁especially', '▁to', '▁the', '▁rapport', 'eur', ',', '▁Mr', '▁Koch', '.', '▁', '</s>']\n",
      "['fr_XX', '▁I', '▁would', '▁like', '▁to', '▁reitera', 'te', '▁my', '▁thanks', '▁to', '▁all', '▁the', '▁speaker', 's', '▁and', '▁in', '▁particular', '▁to', '▁the', '▁rapport', 'eur', ',', '▁Mr', '▁Koch', '.', '▁', '</s>']\n",
      "\n",
      "['fr_XX', '▁The', '▁debate', '▁is', '▁closed', '.', '▁', '</s>']\n",
      "['fr_XX', '▁The', '▁debate', '▁is', '▁closed', '.', '▁', '</s>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We add a sanity check to make sure we have the same amount of references and hypotheses. We also print out the number\n",
    "# of sentences that share the same length. \n",
    "assert len(references) == len(translations)\n",
    "for ref, hyp in zip(references, hypotheses):\n",
    "    if len(ref) == len(hyp):\n",
    "        print(ref)\n",
    "        print(hyp)\n",
    "        print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5254438794181694\n",
      "0.39037920561827\n"
     ]
    }
   ],
   "source": [
    "# these results are with only 45 sentences\n",
    "corpus_references = [[ref] for ref in references]\n",
    "smoothing = nltk.translate.bleu_score.SmoothingFunction().method5  # methods 2 and 5 provide the highest bleu\n",
    "bleu = nltk.translate.bleu_score.corpus_bleu(corpus_references, hypotheses, smoothing_function=smoothing)\n",
    "print(bleu)\n",
    "\n",
    "smoothing = nltk.translate.bleu_score.SmoothingFunction().method2  # methods 2 and 5 provide the highest bleu\n",
    "bleu = nltk.translate.bleu_score.corpus_bleu(corpus_references, hypotheses, smoothing_function=smoothing)\n",
    "print(bleu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence ['fr_XX', '▁Les', '▁rapport', 's', '▁economi', 'ques', '▁dans', '▁le', '▁monde', '▁reel', '▁de', 'mont', 'rent', '▁a', '▁suffi', 's', 'ance', '▁que', '▁l', \"'\", 'eli', 'mination', '▁de', '▁toute', '▁intervention', '▁publique', '▁dans', '▁le', '▁marche', '▁ne', '▁produit', '▁null', 'ement', '▁une', '▁concurrence', '▁parfait', 'e', '▁et', '▁une', '▁al', 'location', '▁optimale', '▁des', '▁ressources', '.', '▁', '</s>']\n",
      "\n",
      "Manual translation from Eurparl: ['fr_XX', '▁Economic', '▁relationships', '▁in', '▁the', '▁real', '▁world', '▁adequat', 'ely', '▁demonstrat', 'e', '▁that', '▁elimina', 'ting', '▁all', '▁public', '▁intervention', '▁in', '▁the', '▁market', '▁does', '▁not', '▁in', '▁any', '▁way', '▁bring', '▁about', '▁perfect', '▁competition', '▁and', '▁the', '▁optim', 'um', '▁distribution', '▁of', '▁resources', '.', '▁', '</s>']\n",
      "\n",
      "mBART translation ['fr_XX', '▁Economic', '▁reports', '▁in', '▁the', '▁real', '▁world', '▁show', '▁that', '▁it', '▁is', '▁', 'sufficient', '▁that', '▁the', '▁elimina', 'tion', '▁of', '▁any', '▁public', '▁intervention', '▁in', '▁the', '▁market', '▁does', '▁not', '▁produce', '▁perfect', '▁competition', '▁and', '▁optimal', '▁al', 'location', '▁of', '▁resources', '.', '</s>']\n",
      "\n",
      "\n",
      "\n",
      "Original sentence ['fr_XX', '▁Si', '▁le', '▁marche', '▁a', '▁et', 'e', '▁depuis', '▁la', '▁nuit', '▁des', '▁temps', '▁le', '▁lieu', '▁d', \"'\", 'e', 'change', '▁privilegi', 'e', '▁des', '▁humain', 's', ',', '▁il', '▁n', \"'\", 'a', '▁jamais', '▁et', 'e', '▁parfait', '.', '▁', '</s>']\n",
      "\n",
      "Manual translation from Eurparl: ['fr_XX', '▁While', ',', '▁since', '▁the', '▁daw', 'n', '▁of', '▁time', ',', '▁the', '▁market', '▁has', '▁been', '▁the', '▁key', '▁forum', '▁for', '▁human', '▁inter', 'change', ',', '▁it', '▁has', '▁never', '▁been', '▁perfect', '.', '▁', '</s>']\n",
      "\n",
      "mBART translation ['fr_XX', '▁If', '▁walking', '▁has', '▁been', '▁the', '▁prefer', 'red', '▁place', '▁of', '▁exchange', '▁for', '▁humans', '▁since', '▁the', '▁night', '▁of', '▁time', ',', '▁it', '▁has', '▁never', '▁been', '▁perfect', '.', '</s>']\n",
      "\n",
      "\n",
      "\n",
      "Original sentence ['fr_XX', '▁Le', '▁marche', '▁privilegi', 'e', '▁le', '▁court', '▁terme', '▁et', '▁les', '▁gain', 's', '▁im', 'media', 'ts', '.', '▁', '</s>']\n",
      "\n",
      "Manual translation from Eurparl: ['fr_XX', '▁The', '▁market', '▁f', 'avour', 's', '▁the', '▁short', '▁term', '▁and', '▁immediate', '▁profit', 's', '.', '▁', '</s>']\n",
      "\n",
      "mBART translation ['fr_XX', '▁The', '▁march', '▁is', '▁a', '▁short', '-', 'term', '▁and', '▁immediate', '▁gain', '.', '</s>']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if we increase the data size to 200 translated sentences then...\n",
    "\n",
    "hypotheses = [tokenizer(sent, return_tensors=\"pt\") for sent in translations]  # do I need the tensors?\n",
    "hypotheses = iterate_tensors(hypotheses)\n",
    "references = eval_sents_from_ids\n",
    "original_french_sents = src_sents_from_ids\n",
    "\n",
    "for i in range(150, 153):  # since we used the test split, i.e. 20% of total data set (1000), we only have 200 sentences.\n",
    "    print(f\"Original sentence {original_french_sents[i]}\" + '\\n')\n",
    "    print(f\"Manual translation from Eurparl: {references[i]}\" + '\\n' * 2 + f\"mBART translation {hypotheses[i]}\" + '\\n' * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "assert len(references) == len(translations)  # sanity check passed.\n",
    "proper_aligned_ref_hyp = 0\n",
    "for ref, hyp in zip(references, hypotheses):\n",
    "    if len(ref) == len(hyp):\n",
    "        proper_aligned_ref_hyp += 1\n",
    "print(proper_aligned_ref_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more sentences to the test set did not improve the bleu score... Probably because only 23 out of 200 sentences have the same length as their references. This means only 11.5% of total test sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4194307038995439\n",
      "0.2945431298371375\n"
     ]
    }
   ],
   "source": [
    "corpus_references = [[ref] for ref in references]\n",
    "smoothing = nltk.translate.bleu_score.SmoothingFunction().method5  # methods 2 and 5 provide the highest bleu\n",
    "bleu = nltk.translate.bleu_score.corpus_bleu(corpus_references, hypotheses, smoothing_function=smoothing)\n",
    "print(bleu)\n",
    "\n",
    "smoothing = nltk.translate.bleu_score.SmoothingFunction().method2  # methods 2 and 5 provide the highest bleu\n",
    "bleu = nltk.translate.bleu_score.corpus_bleu(corpus_references, hypotheses, smoothing_function=smoothing)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences were originally loaded as tuples for the sake of efficiency. However, when we fed them to tokenizer.as_target_tokenizer, issues come up when casting them to tensors. As a result of this, they have been cast to lists before being passed to the tokenizer and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_train_sents = list(trg_train_sents)\n",
    "src_train_sents = list(src_train_sents)\n",
    "\n",
    "trg_test_sents = list(trg_test_sents)\n",
    "src_test_sents = list(src_test_sents)\n",
    "\n",
    "model_inputs = tokenizer(src_train_sents, return_tensors='pt', padding=True)\n",
    "\n",
    "def get_target_sent_info(trg_sents):\n",
    "    \"\"\"\n",
    "    :param trg sents: target sentences could be from train, test, or validation set\n",
    "    returns trg_tokenizer_output with the attention mask that we need, the non-\n",
    "    \"\"\"\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        trg_tokenizer_output = tokenizer(trg_sents, return_tensors='pt', padding=True)    \n",
    "    label_ids = trg_tokenizer_output['input_ids']\n",
    "    pad_mask = trg_tokenizer_output['attention_mask'] == 0    \n",
    "    # using padding mask to convert pad tokens to -100, careful when converting these ids to tokens.\n",
    "    # negative numbers are not part of mBART's ids.\n",
    "    label_ids = torch.where(pad_mask, torch.full_like(label_ids, -100), label_ids)\n",
    "    # concatenaing the eos_taken in the first index (col) per tensor   \n",
    "    label_ids = torch.concat((torch.full((label_ids.shape[0], 1), tokenizer.eos_token_id), label_ids), 1) \n",
    "    return label_ids\n",
    "\n",
    "label_ids = get_target_sent_info(trg_train_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop as explained on [hugging face](https://huggingface.co/docs/transformers/model_doc/mbart) involves passing the model inputs and the labels, which we can visualize in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri May 27 14:57:35 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.68.01    Driver Version: 512.59       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   45C    P8    N/A /  N/A |      0MiB /  4096MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#model_inputs = model_inputs.to(device)\n",
    "#torch.cuda.empty_cache() uncomment if using gpu\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch error\n",
      "1 1.2499650716781616\n",
      "2 0.5504726767539978\n",
      "3 0.3144689202308655\n",
      "4 0.23181627690792084\n",
      "5 0.09578200429677963\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': 2e-5}])  # 0.001\n",
    "minibatch_size = 8\n",
    "length_train_x = input_ids.shape[0]\n",
    "num_minibatches = int(np.ceil(length_train_x/minibatch_size))\n",
    "model.to(device)\n",
    "model.train(True)\n",
    "print('epoch', 'error')\n",
    "for epoch in range(1, 5+1):\n",
    "    indexes = np.arange(length_train_x)\n",
    "    np.random.shuffle(indexes)            \n",
    "    for i in range(num_minibatches):\n",
    "        minibatch_indexes = indexes[i*minibatch_size:(i+1)*minibatch_size]\n",
    "        # in case of GPU, uncomment and make sure device == cuda!\n",
    "        #minibatch_x = torch.tensor(input_ids[minibatch_indexes], dtype=torch.long, device=device)          \n",
    "        #minibatch_y = torch.tensor(label_ids[minibatch_indexes], dtype=torch.long, device=device)        \n",
    "        optimizer.zero_grad()     \n",
    "        #loss = model(minibatch_x, labels=minibatch_y).loss   \n",
    "        loss = model(model_inputs[\"input_ids\"][minibatch_indexes], labels=label_ids[minibatch_indexes]).loss   \n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    if epoch%1 == 0:\n",
    "        print(epoch, loss.detach().tolist())\n",
    "model.train(False)\n",
    "\n",
    "torch.save(model, \"pretrained_model.pth\")\n",
    "bart_model = torch.load(\"pretrained_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss running for 15 epochs and training on 180 sentences on google colab. Minibatch_size == 8 and learning_rate == 2e-5\n",
    "```Python\n",
    "epoch error\n",
    "1 1.8225083351135254\n",
    "2 1.0663105249404907\n",
    "3 0.7641125917434692\n",
    "4 0.5679468512535095\n",
    "5 0.46091803908348083\n",
    "6 0.3314163088798523\n",
    "7 0.29134687781333923\n",
    "8 0.2458421289920807\n",
    "9 0.20358489453792572\n",
    "10 0.18593527376651764\n",
    "11 0.12719759345054626\n",
    "12 0.11407511681318283\n",
    "13 0.11325351893901825\n",
    "14 0.11906187981367111\n",
    "15 0.08755840361118317\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_translations = translate(src, model=bart_model)\n",
    "\n",
    "with open(\"fine_trans_45_sents.txt\", 'w') as f:\n",
    "    for trans in finetuned_translations:        \n",
    "        f.write(trans[0] + '\\n')  # writelines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_hypotheses = [tokenizer(sent, return_tensors=\"pt\") for sent in finetuned_translations]\n",
    "fine_tuned_hypotheses = iterate_tensors(fine_tuned_hypotheses)\n",
    "for i in range(4):\n",
    "    print(f\"Manual translation from Eurparl: {references[i]}\" + '\\n' * 2 + f\"mBART translation {hypotheses[i]}\" + \"\\n\" * 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_references = [[ref] for ref in references]\n",
    "smoothing = nltk.translate.bleu_score.SmoothingFunction().method5  # methods 2 and 5 provide the highest bleu\n",
    "bleu = nltk.translate.bleu_score.corpus_bleu(corpus_references, hypotheses, smoothing_function=smoothing)\n",
    "print(bleu)\n",
    "\n",
    "smoothing = nltk.translate.bleu_score.SmoothingFunction().method2  # methods 2 and 5 provide the highest bleu\n",
    "bleu = nltk.translate.bleu_score.corpus_bleu(corpus_references, hypotheses, smoothing_function=smoothing)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we used the 200 total data set to translate 45 sentences. The training loop has an added minibatch loop to facilitate training, a learning rate of 2e-5 as the learning rate should remain small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MBartForConditionalGeneration.from_pretrained(base_model)\n",
    "\n",
    "# with torch.no_grad():        \n",
    "#     loss = model(**trg_tokenizer_output, labels=label_ids).loss\n",
    "#     logits = model(indexed_val_x, mask_val_x)  # should we use dataloader?\n",
    "#     batch_probs = torch.softmax(logits, dim=1)\n",
    "#     for (indexes, probs) in zip(indexed_val_x, batch_probs):\n",
    "#         target = np.array(indexed_val_y, np.int64)\n",
    "#         output = probs.detach().numpy().argmax(axis=1)\n",
    "#         accuracy = (target == output).sum() / len(target)        \n",
    "#         #print(probs.detach().numpy().argmax(axis=1))\n",
    "#         print(tokenizer.convert_ids_to_tokens(indexes))\n",
    "#         print()\n",
    "#         print([tag_set[index] for index in probs.numpy().argmax(1).tolist()])\n",
    "#         print(accuracy)\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_references = [[ref] for ref in references]\n",
    "smoothing = nltk.translate.bleu_score.SmoothingFunction().method2 \n",
    "bleu = nltk.translate.bleu_score.corpus_bleu(corpus_references, hypotheses, smoothing_function=smoothing)  # hypothesis should be the google translate. reference replaced by example\n",
    "print(bleu)\n",
    "\n",
    "\n",
    "FROM COLAB\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\\* not previously linked\n",
    "- https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/\n",
    "- https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/translation.ipynb\n",
    "- https://tmramalho.github.io/science/2020/06/10/fine-tune-neural-translation-models-with-mBART/\n",
    "- https://huggingface.co/blog/how-to-train\n",
    "- https://zhuanlan.zhihu.com/p/379071787\n",
    "- https://huggingface.co/docs/transformers/training\n",
    "- https://towardsdatascience.com/mbart50-multilingual-fine-tuning-of-extensible-multilingual-pretraining-70a7305d4838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
